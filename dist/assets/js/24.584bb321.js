(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{280:function(e,n,t){},299:function(e,n,t){"use strict";t(280)},316:function(e,n,t){"use strict";t.r(n);t(299);var r=t(14),a=Object(r.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),n("p",[e._v("As a final year Ph.D. student in the Department of Computer Science at the University of Hong Kong, I am conducting research under the supervision of "),n("a",{attrs:{href:"luoping.me"}},[e._v("Prof. Ping Luo")]),e._v(" and "),n("a",{attrs:{href:"https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Wenping Wang"),n("OutboundLink")],1),e._v(". My academic journey began with a foundation in EE from Shenzhen University, under the guidance of "),n("a",{attrs:{href:"dilincv.github.io"}},[e._v("Prof. Lin Di")]),e._v(".")]),e._v(" "),n("p",[e._v("My research interests are currently centered in the field of AI, with a special focus on its application in medicine, with the goal of developing systems that are not only powerful, but also trustworthy.")]),e._v(" "),n("p",[e._v("Inspired by pioneers in the field, my work seeks to contribute to the potential of AI to improve "),n("strong",[e._v("healthcare diagnostics")]),e._v(" and "),n("strong",[e._v("treatment strategies")]),e._v(". The goal is to advance the intersection of technology and healthcare, enabling improved patient outcomes through the application of AI.")]),e._v(" "),n("p",[e._v("Please feel free to contact us via email at u3008013@connect.hku.hk.")]),e._v(" "),n("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),n("ul",[n("li",[e._v("[2024-01] AutoBench is accepted by ICLR24. See you in Wien")]),e._v(" "),n("li",[e._v("[2023-11] I joint the "),n("a",{attrs:{href:"https://scholar.google.com/citations?user=Y89JnCYAAAAJ&hl=en",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof. Ruijiang Li"),n("OutboundLink")],1),e._v("'s group at Stanford University as a visiting student.")]),e._v(" "),n("li",[e._v("[2023-07] DDP is accepted by ICCV23. See you in Paris")]),e._v(" "),n("li",[e._v("[2022-11] DrugOOD is accepted by AAAI23.")]),e._v(" "),n("li",[e._v("[2022-10] AMOS is accepted by NIPS22. See you in New Orleans")])]),e._v(" "),n("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://scholar.google.com/citations?user=8Z3Z5YIAAAAJ&hl=en",target:"_blank",rel:"noopener noreferrer"}},[e._v("â†’ Full list"),n("OutboundLink")],1)]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2023/padded_auto-bench.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Large Language Models as Automated Aligners for benchmarking Vision-Language Models")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v("*, Chongjian Ge*, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo")]),e._v(" "),n("p",[e._v("ICLR 2024")]),e._v(" "),n("p",[e._v("Introduction: This research explores the potential of large language models as automated aligners, setting a new benchmark in vision-language model evaluation.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2311.14580.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://jiyuanfeng.github.io/auto-bench.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code&Data(wip)"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2023/padded_syncdock.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("SyNDock: N Rigid Protein Docking via Learnable Transformation Synchronization")])]),e._v(" "),n("p",[e._v("Yuanfeng Ji, Yatao Bian, Guoji Fu, Peilin Zhao, Ping Luo")]),e._v(" "),n("p",[e._v("Tech report")]),e._v(" "),n("p",[e._v("Introduction: SyNDock presents an innovative approach to protein docking, utilizing learnable transformation synchronization for enhanced accuracy and efficiency.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2305.15156v1.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:""}},[e._v("Code&Data(wip)")]),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2023/padded_ddp.png  ",hideBorder:"true"}},[n("p",[n("strong",[e._v("DDP: Diffusion Model for Dense Visual Prediction")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v("*, Zhe Chen*, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo")]),e._v(" "),n("p",[e._v("ICCV 2023")]),e._v(" "),n("p",[e._v("Introduction: A groundbreaking approach to dense visual prediction, employing diffusion models to enhance accuracy and efficiency.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2303.17559",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/JiYuanFeng/DDP",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2021/padded_drugood.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v("*, Lu Zhang*, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang, Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, Houtim Lai, Wei Liu, Junzhou Huang, Shuigeng Zhou, Ping Luo, Peilin Zhao, Yatao Bian")]),e._v(" "),n("p",[e._v("AAAI 2023 (Oral)")]),e._v(" "),n("p",[e._v("Introduction: DrugOOD serves as a curator and benchmark for AI-driven drug discovery, focusing on affinity prediction problems with noise annotations.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/abs/2201.09637",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/tencent-ailab/DrugOOD",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://drugood.github.io",target:"_blank",rel:"noopener noreferrer"}},[e._v("Project"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2022/padded_amos.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v(", Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo")]),e._v(" "),n("p",[e._v("Neuips 2022 (Oral)")]),e._v(" "),n("p",[e._v("Introduction: AMOS stands as a large-scale benchmark for abdominal multi-organ segmentation, paving the way for advancements in medical image analysis.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2206.08023.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/JiYuanFeng/AMOS/tree/code",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://amos22.grand-challenge.org",target:"_blank",rel:"noopener noreferrer"}},[e._v("Challenge"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2021/padded_miccai01.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("Multi-compound Transformer for Accurate Biomedical Image Segmentation")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v(", Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting Zhang, Ping Luo")]),e._v(" "),n("p",[e._v("MICCAI 2021 (Early Accept)")]),e._v(" "),n("p",[e._v("Introduction: This work introduces a transformative approach in biomedical image segmentation, leveraging a multi-compound transformer architecture for enhanced accuracy.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2106.14385",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/JiYuanFeng/MCTrans",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2020/padded_miccai01.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v(", Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping Luo")]),e._v(" "),n("p",[e._v("MICCAI 2021 (Early Accept)")]),e._v(" "),n("p",[e._v("Introduction: UXNet propels the search for multi-level feature aggregation in 3D medical image segmentation through an AutoML tool for network design.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/2009.07501",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] [[Code(coming)]]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2019/padded_miccai01.png",hideBorder:"true"}},[n("p",[n("strong",[e._v("PRSNet: Part Relation and Selection Network For Bone Age Assessment")])]),e._v(" "),n("p",[n("strong",[e._v("Yuanfeng Ji")]),e._v(", Hao Chen, Dan Lin, Xiaohua Wu, Di Lin")]),e._v(" "),n("p",[e._v("MICCAI 2020 (Early Accept)")]),e._v(" "),n("p",[e._v("Introduction: PRSNet innovates bone age assessment by integrating part relation and selection networks to streamline the analysis process.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://arxiv.org/pdf/1909.056515",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2020/padded_zigzag.png",hideBorder:"false"}},[n("p",[n("strong",[e._v("RANet: Region Attention Network for Semantic Segmentation")])]),e._v(" "),n("p",[e._v("Dingguo Shen*, Yuanfeng Ji*, Ping Li, Yi Wang, Di Lin")]),e._v(" "),n("p",[e._v("Neuips 2020")]),e._v(" "),n("p",[e._v("Introduction: RANet leverages region-based attention mechanisms to enhance the performance of semantic segmentation tasks.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://dilincv.github.io/ranet.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://github.com/dingguo1996/RANet",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("ProjectCard",{attrs:{image:"/projects/2019/padded_eccv1.jpg",hideBorder:"true"}},[n("p",[n("strong",[e._v("Multi-Scale Context Interwining for Semantic Segmentation")])]),e._v(" "),n("p",[e._v("Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, Hui Huang")]),e._v(" "),n("p",[e._v("ECCV 2018")]),e._v(" "),n("p",[e._v("Introduction: MSCI introduces an innovative approach to semantic segmentation by intertwining multi-scale contextual information, enhancing the accuracy and robustness of the segmentation process.")]),e._v(" "),n("p",[e._v("["),n("a",{attrs:{href:"https://dilincv.github.io/msci.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v("] ["),n("a",{attrs:{href:"https://vcc.tech/msci",target:"_blank",rel:"noopener noreferrer"}},[e._v("Project"),n("OutboundLink")],1),e._v("]")])]),e._v(" "),n("h2",{attrs:{id:"challenges-achievements"}},[e._v("Challenges & Achievements")]),e._v(" "),n("ul",[n("li",[e._v("[2019] Ranked 5th in the "),n("a",{attrs:{href:"https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/leaderboard",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaggle RSNA Pneumonia Detection Challenge"),n("OutboundLink")],1),e._v(" (Gold Medal).")]),e._v(" "),n("li",[e._v("[2019] Ranked 3rd in the "),n("a",{attrs:{href:"https://competitions.codalab.org/competitions/19507#results",target:"_blank",rel:"noopener noreferrer"}},[e._v("COCO 2019 Panoptic Segmentation Task"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2018] Ranked 3rd in the "),n("a",{attrs:{href:"https://challenge.isic-archive.com/leaderboards/2018/",target:"_blank",rel:"noopener noreferrer"}},[e._v("MICCAI 2018 ISIC Skin Lesion Segmentation Challenge"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[2019] Ranked 87th in the "),n("a",{attrs:{href:"https://www.kaggle.com/competitions/human-protein-atlas-image-classification/leaderboard",target:"_blank",rel:"noopener noreferrer"}},[e._v("Kaggle Human Protein Atlas Image Classification Challenge"),n("OutboundLink")],1),e._v(" (Silver Medal).")])]),e._v(" "),n("h2",{attrs:{id:"professional-activities"}},[e._v("Professional Activities")]),e._v(" "),n("ul",[n("li",[e._v("Organizer for the "),n("a",{attrs:{href:"https://amos22.grand-challenge.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Multi-Modality Abdominal Multi-Organ Segmentation Challenge"),n("OutboundLink")],1),e._v(" at MICCAI 2022.")]),e._v(" "),n("li",[e._v("Journal Reviewer for TMI, TMM.")]),e._v(" "),n("li",[e._v("Conference Reviewer for CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, MICCAI.")])]),e._v(" "),n("h2",{attrs:{id:"experience"}},[e._v("Experience")]),e._v(" "),n("p",[e._v("I am deeply grateful for the growth and learning I have experienced under the guidance of my respected mentors.")]),e._v(" "),n("ul",[n("li",[e._v("[2023-11 ~ Present] Visiting Student at Stanford University, advised by "),n("a",{attrs:{href:"https://scholar.google.com/citations?user=Y89JnCYAAAAJ&hl=en",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof.Ruijiang Li"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("[2023-04 ~ 2023-11] Research Intern at Huawei Noah's Ark Lab, advised by "),n("a",{attrs:{href:"https://xieenze.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr.Enze Xie"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("[2021-04 ~ 2023-04] Research Intern at Tencent AI Lab, advised by "),n("a",{attrs:{href:"https://yataobian.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr.Yatao Bian"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("[2019-07 ~ 2020-10] Research Intern at SenseTime Research, advised by "),n("a",{attrs:{href:"http://zhangruimao.site/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof.Ruimao Zhang"),n("OutboundLink")],1)]),e._v(" "),n("li",[e._v("[2018-05 ~ 2019-06] Research Intern at Imsight Medical Technology, advised by "),n("a",{attrs:{href:"https://scholar.google.com.hk/citations?user=Z_t5DjwAAAAJ&hl=zh-TW",target:"_blank",rel:"noopener noreferrer"}},[e._v("Prof.Hao Chen"),n("OutboundLink")],1)])])],1)}),[],!1,null,null,null);n.default=a.exports}}]);