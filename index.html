<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>VuePress</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="The description of the site.">
    
    <link rel="preload" href="/assets/css/0.styles.987a689f.css" as="style"><link rel="preload" href="/assets/js/app.a9e1ec78.js" as="script"><link rel="preload" href="/assets/js/2.87855bce.js" as="script"><link rel="preload" href="/assets/js/1.4afc779d.js" as="script"><link rel="preload" href="/assets/js/24.31e93cbe.js" as="script"><link rel="preload" href="/assets/js/20.f657c739.js" as="script"><link rel="preload" href="/assets/js/21.840938a7.js" as="script"><link rel="prefetch" href="/assets/js/10.026a8577.js"><link rel="prefetch" href="/assets/js/11.0bfc8210.js"><link rel="prefetch" href="/assets/js/12.de255aa7.js"><link rel="prefetch" href="/assets/js/13.9d8e2296.js"><link rel="prefetch" href="/assets/js/14.0912cb8f.js"><link rel="prefetch" href="/assets/js/15.436c174e.js"><link rel="prefetch" href="/assets/js/16.d74740b1.js"><link rel="prefetch" href="/assets/js/17.eb05b18f.js"><link rel="prefetch" href="/assets/js/18.4c170722.js"><link rel="prefetch" href="/assets/js/19.cd20d3a2.js"><link rel="prefetch" href="/assets/js/22.5f831f68.js"><link rel="prefetch" href="/assets/js/23.e7191542.js"><link rel="prefetch" href="/assets/js/25.7925cad4.js"><link rel="prefetch" href="/assets/js/26.b5a21033.js"><link rel="prefetch" href="/assets/js/3.d05ac253.js"><link rel="prefetch" href="/assets/js/4.a26f2c89.js"><link rel="prefetch" href="/assets/js/5.f0d2deb5.js"><link rel="prefetch" href="/assets/js/6.a59e9075.js"><link rel="prefetch" href="/assets/js/7.4da40eac.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.969cea78.js">
    <link rel="stylesheet" href="/assets/css/0.styles.987a689f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" aria-current="page" class="home-link router-link-exact-active router-link-active"></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" aria-current="page" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/person.jpeg" alt></div> <div class="info"><div class="name">
      Yuanfeng Ji (纪源丰)
    </div> <div class="bio"><p></p></div> <div class="socials"><div><a href="https://github.com/JiYuanFeng" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div><div><a href="u3008013@connect.hku.hk" target="_blank"><img src="/icons/email.svg" alt="email" title="email"></a></div><div><a href="https://www.linkedin.com/in/yuanfeng-ji-7b0b3a1b0/" target="_blank"><img src="/icons/linkedin.svg" alt="linkedin" title="linkedin"></a></div><div><a href="https://scholar.google.com/citations?user=8Z3Z5YIAAAAJ&amp;hl=en" target="_blank"><img src="/icons/gscholar.svg" alt="google-scholar" title="google-scholar"></a></div><div><a href="/pdf/cv.pdf" target="_blank"><img src="/icons/cv.svg" alt="resume" title="resume"></a></div></div> <div class="contact"><div title="Contact me" class="email">u3008013@connect.hku.hk</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>As a final year Ph.D. student in the Department of Computer Science at the University of Hong Kong, I am engaged in cutting-edge research under the supervision of <a href="luoping.me">Prof. Ping Luo</a> and <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html" target="_blank" rel="noopener noreferrer">Prof. Wenping Wang<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. My academic journey began with a solid foundation in EE from Shenzhen University, nurtured by the guidance of <a href="dilincv.github.io">Prof. Lin Di</a>.</p> <p>My scientific pursuits are currently anchored in the field of AI, with a special focus on its ethical integration into medicine, aiming to develop systems that are not only powerful but also trustworthy.</p> <p>Inspired by luminaries in the field, my work seeks to contribute to the transformative potential of AI in improving <strong>healthcare diagnostics</strong> and <strong>treatment strategies</strong>. The goal is to advance the intersection of technology and healthcare, enabling improved patient outcomes through innovative AI applications.</p> <p>Please feel free to contact us via email at u3008013@connect.hku.hk.</p> <h2 id="news">News</h2> <ul><li>[2023-11] I joint the <a href="https://scholar.google.com/citations?user=Y89JnCYAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Ruijiang Li<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>'s group at Stanford University as a visiting student.</li> <li>[2023-07] DDP is accepted by ICCV23. See you in Paris</li> <li>[2022-11] DrugOOD is accepted by AAAI23.</li> <li>[2022-10] AMOS is accepted by NIPS22. See you in New Orleans</li></ul> <h2 id="experience">Experience</h2> <p>I am deeply grateful for the growth and learning I've experienced under the guidance of my esteemed mentors.</p> <ul><li>[2023-11 ~ Present] Visiting Student at Stanford University, advised by <a href="https://scholar.google.com/citations?user=Y89JnCYAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Ruijiang Li<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>[2023-04 ~ 2023-11] Research Intern at Huawei Noah's Ark Lab, advised by <a href="https://xieenze.github.io/" target="_blank" rel="noopener noreferrer">Enze Xie<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>[2021-04 ~ 2023-04] Research Intern at Tencent AI Lab, advised by <a href="https://yataobian.com/" target="_blank" rel="noopener noreferrer">Yatao Bian<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>[2019-07 ~ 2020-10] Research Intern at SenseTime Research, advised by <a href="http://zhangruimao.site/" target="_blank" rel="noopener noreferrer">Ruimao Zhang<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>[2018-05 ~ 2019-06] Research Intern at Imsight Medical Technology, advised by <a href="https://scholar.google.com.hk/citations?user=Z_t5DjwAAAAJ&amp;hl=zh-TW" target="_blank" rel="noopener noreferrer">Hao Chen<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="publications">Publications</h2> <p><a href="https://scholar.google.com/citations?user=8Z3Z5YIAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">→ Full list<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <div class="md-card"><div class="card-image"><img src="/projects/2023/padded_auto-bench.png" alt></div> <div class="card-content"><p><strong>Large Language Models as Automated Aligners for benchmarking Vision-Language Models</strong></p> <p>Yuanfeng Ji*, Chongjian Ge*, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo</p> <p>Tech report</p> <p>Introduction: This research explores the potential of large language models as automated aligners, setting a new benchmark in vision-language model evaluation.</p> <p>[<a href="https://arxiv.org/pdf/2311.14580.pdf" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://jiyuanfeng.github.io/auto-bench.html" target="_blank" rel="noopener noreferrer">Code&amp;Data(wip)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2023/padded_syncdock.png" alt></div> <div class="card-content"><p><strong>SyNDock: N Rigid Protein Docking via Learnable Transformation Synchronization</strong></p> <p>Yuanfeng Ji, Yatao Bian, Guoji Fu, Peilin Zhao, Ping Luo</p> <p>Tech report</p> <p>Introduction: SyNDock presents an innovative approach to protein docking, utilizing learnable transformation synchronization for enhanced accuracy and efficiency.</p> <p>[<a href="https://arxiv.org/pdf/2305.15156v1.pdf" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="">Code&amp;Data(wip)</a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2023/padded_ddp.png  " alt></div> <div class="card-content"><p><strong>DDP: Diffusion Model for Dense Visual Prediction</strong></p> <p><strong>Yuanfeng Ji</strong>, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo</p> <p>ICCV 2023</p> <p>Introduction: A groundbreaking approach to dense visual prediction, employing diffusion models to enhance accuracy and efficiency.</p> <p>[<a href="https://arxiv.org/abs/2303.17559" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/JiYuanFeng/DDP" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2021/padded_drugood.png" alt></div> <div class="card-content"><p><strong>DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery</strong></p> <p>Yuanfeng Ji*, Lu Zhang*, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang, Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, Houtim Lai, Wei Liu, Junzhou Huang, Shuigeng Zhou, Ping Luo, Peilin Zhao, Yatao Bian</p> <p>AAAI 2023 (Oral)</p> <p>Introduction: DrugOOD serves as a curator and benchmark for AI-driven drug discovery, focusing on affinity prediction problems with noise annotations.</p> <p>[<a href="https://arxiv.org/abs/2201.09637" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/tencent-ailab/DrugOOD" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://drugood.github.io" target="_blank" rel="noopener noreferrer">Project<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2022/padded_amos.png" alt></div> <div class="card-content"><p><strong>AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation</strong></p> <p><strong>Yuanfeng Ji</strong>, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo</p> <p>Neuips 2022 (Oral)</p> <p>Introduction: AMOS stands as a large-scale benchmark for abdominal multi-organ segmentation, paving the way for advancements in medical image analysis.</p> <p>[<a href="https://arxiv.org/pdf/2206.08023.pdf" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/JiYuanFeng/AMOS/tree/code" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://amos22.grand-challenge.org" target="_blank" rel="noopener noreferrer">Challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2021/padded_miccai01.png" alt></div> <div class="card-content"><p><strong>Multi-compound Transformer for Accurate Biomedical Image Segmentation</strong></p> <p><strong>Yuanfeng Ji</strong>, Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting Zhang, Ping Luo</p> <p>MICCAI 2021 (Early Accept)</p> <p>Introduction: This work introduces a transformative approach in biomedical image segmentation, leveraging a multi-compound transformer architecture for enhanced accuracy.</p> <p>[<a href="https://arxiv.org/pdf/2106.14385" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/JiYuanFeng/MCTrans" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2020/padded_miccai01.png" alt></div> <div class="card-content"><p><strong>UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation</strong></p> <p><strong>Yuanfeng Ji</strong>, Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping Luo</p> <p>MICCAI 2021 (Early Accept)</p> <p>Introduction: UXNet propels the search for multi-level feature aggregation in 3D medical image segmentation through an AutoML tool for network design.</p> <p>[<a href="https://arxiv.org/pdf/2009.07501" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [[Code(coming)]]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2019/padded_miccai01.png" alt></div> <div class="card-content"><p><strong>PRSNet: Part Relation and Selection Network For Bone Age Assessment</strong></p> <p><strong>Yuanfeng Ji</strong>, Hao Chen, Dan Lin, Xiaohua Wu, Di Lin</p> <p>MICCAI 2020 (Early Accept)</p> <p>Introduction: PRSNet innovates bone age assessment by integrating part relation and selection networks to streamline the analysis process.</p> <p>[<a href="https://arxiv.org/pdf/1909.056515" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2020/padded_zigzag.png" alt></div> <div class="card-content"><p><strong>RANet: Region Attention Network for Semantic Segmentation</strong></p> <p>Dingguo Shen*, Yuanfeng Ji*, Ping Li, Yi Wang, Di Lin</p> <p>Neuips 2020</p> <p>Introduction: RANet leverages region-based attention mechanisms to enhance the performance of semantic segmentation tasks.</p> <p>[<a href="https://dilincv.github.io/ranet.pdf" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://github.com/dingguo1996/RANet" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/2019/padded_eccv1.jpg" alt></div> <div class="card-content"><p><strong>Multi-Scale Context Interwining for Semantic Segmentation</strong></p> <p>Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, Hui Huang</p> <p>ECCV 2018</p> <p>Introduction: MSCI introduces an innovative approach to semantic segmentation by intertwining multi-scale contextual information, enhancing the accuracy and robustness of the segmentation process.</p> <p>[<a href="https://dilincv.github.io/msci.pdf" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>] [<a href="https://vcc.tech/msci" target="_blank" rel="noopener noreferrer">Project<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>]</p></div></div> <h2 id="challenges-achievements">Challenges &amp; Achievements</h2> <ul><li>[2019] Ranked 5th in the <a href="https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/leaderboard" target="_blank" rel="noopener noreferrer">Kaggle RSNA Pneumonia Detection Challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> (Gold Medal).</li> <li>[2019] Ranked 3rd in the <a href="https://competitions.codalab.org/competitions/19507#results" target="_blank" rel="noopener noreferrer">COCO 2019 Panoptic Segmentation Task<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[2018] Ranked 3rd in the <a href="https://challenge.isic-archive.com/leaderboards/2018/" target="_blank" rel="noopener noreferrer">MICCAI 2018 ISIC Skin Lesion Segmentation Challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[2019] Ranked 87th in the <a href="https://www.kaggle.com/competitions/human-protein-atlas-image-classification/leaderboard" target="_blank" rel="noopener noreferrer">Kaggle Human Protein Atlas Image Classification Challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> (Silver Medal).</li></ul> <h2 id="professional-activities">Professional Activities</h2> <ul><li>Organizer for the <a href="https://amos22.grand-challenge.org/" target="_blank" rel="noopener noreferrer">Multi-Modality Abdominal Multi-Organ Segmentation Challenge<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> at MICCAI 2022.</li> <li>Journal Reviewer for TMI, TMM.</li> <li>Conference Reviewer for CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, MICCAI.</li></ul></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.a9e1ec78.js" defer></script><script src="/assets/js/2.87855bce.js" defer></script><script src="/assets/js/1.4afc779d.js" defer></script><script src="/assets/js/24.31e93cbe.js" defer></script><script src="/assets/js/20.f657c739.js" defer></script><script src="/assets/js/21.840938a7.js" defer></script>
  </body>
</html>
