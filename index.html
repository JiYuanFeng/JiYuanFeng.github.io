<!DOCTYPE html><html lang="en" class="__className_25a186"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/JiYuanFeng.github.io/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/JiYuanFeng.github.io/_next/static/css/d64a1b622cbccaf5.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/JiYuanFeng.github.io/_next/static/chunks/webpack-a3b8e77fa4d566ad.js"/><script src="/JiYuanFeng.github.io/_next/static/chunks/fd9d1056-023bbdd6fd8a5298.js" async=""></script><script src="/JiYuanFeng.github.io/_next/static/chunks/23-83ce6e52f2010472.js" async=""></script><script src="/JiYuanFeng.github.io/_next/static/chunks/main-app-05bfd35889b672e2.js" async=""></script><script src="/JiYuanFeng.github.io/_next/static/chunks/193-5cbb6b05bc1adb61.js" async=""></script><script src="/JiYuanFeng.github.io/_next/static/chunks/app/page-207b02275482d296.js" async=""></script><script src="/JiYuanFeng.github.io/_next/static/chunks/app/layout-81ae2f8f2c0f9613.js" async=""></script><title>Yuanfeng Ji | Researcher specializing in artificial intelligence applications in medicine, focused on advancing reliable and impactful healthcare technologies.</title><meta name="description" content="As a postdoctoral researcher at Stanford University, I am dedicated to pioneering AI solutions in medical imaging and digital pathology. My work is driven by a commitment to developing robust and trustworthy systems that can contribute to advancements in healthcare diagnostics and treatment planning."/><link rel="icon" href="/JiYuanFeng.github.io/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="apple-touch-icon" href="/JiYuanFeng.github.io/apple-icon.png?d70e5c41a56c84c2" type="image/png" sizes="180x180"/><meta name="next-size-adjust"/><script src="/JiYuanFeng.github.io/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js" noModule=""></script></head><body><main class="container relative mx-auto scroll-my-12 overflow-auto p-4 print:p-12 md:p-16"><section class="mx-auto w-full max-w-2xl space-y-8 bg-white print:space-y-4"><div class="flex items-center justify-between"><div class="flex-1 space-y-1.5"><h1 class="text-2xl font-bold">Yuanfeng Ji</h1><p class="max-w-md text-pretty font-mono text-sm text-muted-foreground print:text-[12px]">Researcher specializing in artificial intelligence applications in medicine, focused on advancing reliable and impactful healthcare technologies.</p><p class="max-w-md items-center text-pretty font-mono text-xs text-muted-foreground"><a class="inline-flex gap-x-1.5 align-baseline leading-none hover:underline" href="https://www.google.com/maps/place/California" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-globe size-3"><circle cx="12" cy="12" r="10"></circle><path d="M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20"></path><path d="M2 12h20"></path></svg>California, USA, PT</a></p><div class="flex gap-x-1 pt-1 font-mono text-sm text-muted-foreground print:hidden"><a href="mailto:yfj@stanford.edu" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground size-8"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-mail size-4"><rect width="20" height="16" x="2" y="4" rx="2"></rect><path d="m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7"></path></svg></a><a href="tel:6504441771" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground size-8"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-phone size-4"><path d="M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z"></path></svg></a><a href="https://github.com/JiYuanFeng" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground size-8"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="size-4"><path fill="currentColor" d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a href="https://www.linkedin.com/in/yuanfeng-ji/" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground size-8"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="size-4"><title>LinkedIn</title><path fill="currentColor" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a href="https://x.com/YuanfengJi" class="inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground size-8"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="size-4"><title>X</title><path fill="currentColor" d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"></path></svg></a></div><div class="hidden flex-col gap-x-1 font-mono text-sm text-muted-foreground print:flex print:text-[12px]"><a href="mailto:yfj@stanford.edu"><span class="underline">yfj@stanford.edu</span></a><a href="tel:6504441771"><span class="underline">6504441771</span></a></div></div><span class="relative flex shrink-0 overflow-hidden rounded-xl size-28"><span class="flex h-full w-full items-center justify-center rounded-xl bg-muted">YJ</span></span></div><section class="flex min-h-0 flex-col gap-y-3"><h2 class="text-xl font-bold">About</h2><p class="text-pretty font-mono text-sm text-muted-foreground print:text-[12px]">As a postdoctoral researcher at Stanford University, I am dedicated to pioneering AI solutions in medical imaging and digital pathology. My work is driven by a commitment to developing robust and trustworthy systems that can contribute to advancements in healthcare diagnostics and treatment planning.</p></section><div><section class="flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16"><h2 class="text-xl font-bold">News</h2><div class="relative max-w-3xl mx-auto"> <div class="border-l-2 border-gray-300 ml-4"><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2024-11-11</span><h3 class="text-base font-semibold mt-1"><a href="https://www.xiaohongshu.com/explore/672a20b7000000001b02e05d?app_platform=android&amp;ignoreEngage=true&amp;app_version=8.58.0&amp;share_from_user_hidden=true&amp;xsec_source=app_share&amp;type=normal&amp;xsec_token=CBbHMhqciUrFq55JAzY43nmv_0rOqQQMnJubUEmchn_mw=&amp;author_share=1&amp;xhsshare=WeixinSession&amp;shareRedId=N0pGNUhLRUs2NzUyOTgwNjY0OTc5NUxL&amp;apptime=1730912776&amp;share_id=b0d1e8b70625499aa9c7e1898cd5f932" class="hover:underline" target="_blank" rel="noopener noreferrer">Hiring Internship Positions for Medical AGI Research</a></h3><p class="text-xs mt-1 text-gray-700">Our team is looking for talented interns to join us in exploring Medical AGI. For detailed information, please see the link. If you&#x27;re interested and passionate, feel free to reach out directly!</p></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2024-11-1</span><h3 class="text-base font-semibold mt-1"><a href="https://profiles.stanford.edu/yuanfeng-ji" class="hover:underline" target="_blank" rel="noopener noreferrer">Joined LiLab as a Postdoctoral Researcher</a></h3><p class="text-xs mt-1 text-gray-700">I completed my Ph.D. in August 2024 and have joined Li Lab at Stanford University as a postdoctoral researcher.</p></div></div></div></section></div><section class="flex min-h-0 flex-col gap-y-3"><h2 class="text-xl font-bold">Work Experience</h2><div class="relative"><div class="border-l-2 border-gray-300 ml-4"><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2024<!-- --> - <!-- -->Present</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://profiles.stanford.edu/yuanfeng-ji">Stanford University</a></h3><p class="text-xs mt-1 text-gray-700">Postdoctoral Researcher</p><p class="text-xs mt-1 text-gray-700">Engaged in AI applications for precision medicine under the guidance of Prof. Ruijiang Li.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2023<!-- --> - <!-- -->2024</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://profiles.stanford.edu/yuanfeng-ji">Stanford University</a></h3><p class="text-xs mt-1 text-gray-700">Visiting Student Researcher</p><p class="text-xs mt-1 text-gray-700">Engaged in AI applications for precision medicine under the guidance of Prof. Ruijiang Li.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2022<!-- --> - <!-- -->2023</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://www.noahlab.com.hk/">Huawei Noah&#x27;s Ark Lab</a></h3><p class="text-xs mt-1 text-gray-700">Research Intern</p><p class="text-xs mt-1 text-gray-700">Developed AI models for predicting cancer treatment outcomes, focusing on precision medicine.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2021<!-- --> - <!-- -->2022</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a></h3><p class="text-xs mt-1 text-gray-700">Research Intern</p><p class="text-xs mt-1 text-gray-700">Led the development of a DrugAI dataset and benchmark for out-of-distribution generalization; developed multi-protein docking algorithms incorporating graph-based deep learning techniques.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2019<!-- --> - <!-- -->2020</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://www.sensetime.com/">SenseTime Research</a></h3><p class="text-xs mt-1 text-gray-700">Research Intern</p><p class="text-xs mt-1 text-gray-700">Developed automated machine learning algorithms for medical image analysis; led the creation of a multi-site abdominal organ segmentation dataset and benchmark.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2018<!-- --> - <!-- -->2019</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://www.imsightmed.com/">Imsight Medical Technology</a></h3><p class="text-xs mt-1 text-gray-700">Deep Learning Researcher</p><p class="text-xs mt-1 text-gray-700">Led the development of CAD products implemented in several institutions in Hong Kong, including a chest X-ray diagnostic system detecting 17 lung diseases and a sequencing algorithm optimizing diagnostic queues at medical facilities.</p><div class="flex gap-x-1 mt-1"></div></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2018<!-- --> - <!-- -->2019</span><h3 class="text-base font-semibold mt-1"><a class="hover:underline" href="https://vcc.tech/">Visual Computing Research Center, Shenzhen University</a></h3><p class="text-xs mt-1 text-gray-700">Research Assistant</p><p class="text-xs mt-1 text-gray-700">Under the supervision of Prof. Hui Huang and Prof. Di Lin, contributed to research on semantic segmentation.</p><div class="flex gap-x-1 mt-1"></div></div></div></div></section><section class="flex min-h-0 flex-col gap-y-3"><h2 class="text-xl font-bold">Education</h2><div class="relative"><div class="border-l-2 border-gray-300 ml-4"><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2014<!-- --> - <!-- -->2018</span><h3 class="text-base font-semibold mt-1">Shenzhen University</h3><p class="text-xs mt-1 text-gray-700">Bachelor&#x27;s Degree in Electronic Information Engineering</p></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2019<!-- --> - <!-- -->2020</span><h3 class="text-base font-semibold mt-1">City University of Hong Kong</h3><p class="text-xs mt-1 text-gray-700">Master&#x27;s Degree in Electronic Information Engineering</p></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2020<!-- --> - <!-- -->2022</span><h3 class="text-base font-semibold mt-1">The University of Hong Kong</h3><p class="text-xs mt-1 text-gray-700">MPhil in Computer Science</p></div><div class="relative mb-8 ml-8"><span class="text-xs text-gray-500">2022<!-- --> - <!-- -->2024</span><h3 class="text-base font-semibold mt-1">The University of Hong Kong</h3><p class="text-xs mt-1 text-gray-700">Ph.D. Candidate in Computer Science</p></div></div></div></section><section class="flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16"><h2 class="text-xl font-bold">Projects</h2><div class="-mx-3 grid grid-cols-1 gap-3 print:grid-cols-3 print:gap-2 md:grid-cols-2 lg:grid-cols-3"><div class="rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3"><div class="flex flex-col space-y-1.5"><div class="space-y-1"><h3 class="font-semibold tracking-tight text-base"><a href="https://arxiv.org/abs/2303.17559" target="_blank" class="inline-flex items-center gap-1 hover:underline">DDP: Diffusion Model for Dense Visual Prediction<!-- --> <span class="size-1 rounded-full bg-green-500"></span></a></h3><div class="hidden font-mono text-xs underline print:visible">arxiv.orgabs/2303.17559</div><p class="text-muted-foreground font-mono text-xs print:text-[10px]">Developed a framework for dense visual predictions based on the conditional diffusion pipeline, following a &#x27;noise-to-map&#x27; generative paradigm.</p></div></div><div class="text-pretty font-mono text-sm text-muted-foreground mt-auto flex"><div class="mt-2 flex flex-wrap gap-1"><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Research Project</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Python</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">PyTorch</div></div></div></div><div class="rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3"><div class="flex flex-col space-y-1.5"><div class="space-y-1"><h3 class="font-semibold tracking-tight text-base"><a href="https://neurips.cc/virtual/2022/poster/55771" target="_blank" class="inline-flex items-center gap-1 hover:underline">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark<!-- --> <span class="size-1 rounded-full bg-green-500"></span></a></h3><div class="hidden font-mono text-xs underline print:visible">neurips.ccvirtual/2022/poster/55771</div><p class="text-muted-foreground font-mono text-xs print:text-[10px]">Created a comprehensive benchmark for abdominal multi-organ segmentation, facilitating advancements in medical image analysis.</p></div></div><div class="text-pretty font-mono text-sm text-muted-foreground mt-auto flex"><div class="mt-2 flex flex-wrap gap-1"><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Research Project</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Python</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">TensorFlow</div></div></div></div><div class="rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3"><div class="flex flex-col space-y-1.5"><div class="space-y-1"><h3 class="font-semibold tracking-tight text-base"><a href="https://arxiv.org/abs/2201.09637" target="_blank" class="inline-flex items-center gap-1 hover:underline">DrugOOD: Out-of-Distribution Dataset Curator and Benchmark<!-- --> <span class="size-1 rounded-full bg-green-500"></span></a></h3><div class="hidden font-mono text-xs underline print:visible">arxiv.orgabs/2201.09637</div><p class="text-muted-foreground font-mono text-xs print:text-[10px]">Developed a dataset curator and benchmark for AI-aided drug discovery, focusing on affinity prediction problems with noisy annotations.</p></div></div><div class="text-pretty font-mono text-sm text-muted-foreground mt-auto flex"><div class="mt-2 flex flex-wrap gap-1"><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Research Project</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">Python</div><div class="inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight">PyTorch</div></div></div></div></div></section><section class="flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16"><h2 class="text-xl font-bold">Publications</h2><div class="-mx-3 grid grid-cols-1 gap-3 print:grid-cols-1 print:gap-2 md:grid-cols-1 lg:grid-cols-1"><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2410.20723" class="hover:underline" target="_blank" rel="noopener noreferrer">CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Tech Report</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Chongjian Ge, Chenfeng Xu, Yuanfeng Ji, Chensheng Peng, Masayoshi Tomizuka, Ping Luo, Mingyu Ding, Varun Jampani, Wei Zhan</p><div class="text-pretty"><a href="https://arxiv.org/abs/2410.20723" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://chongjiange.github.io/compgs.html" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->project<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2410.11761" class="hover:underline" target="_blank" rel="noopener noreferrer">SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Tech Report</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Ying Chen*, Guoan Wang*, Yuanfeng Ji*#, Yanjun Li, Jin Ye, Tianbin Li, Bin Zhang, Nana Pei, Rongshan Yu, Yu Qiao, Junjun He#</p><div class="text-pretty"><a href="https://arxiv.org/abs/2410.11761" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2305.15156" class="hover:underline" target="_blank" rel="noopener noreferrer">SyNDock: N Rigid Protein Docking via Learnable Group Synchronization</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Tech Report</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji, Yatao Bian, Guoji Fu, Peilin Zhao, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2305.15156" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/SyNDock" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a><a href="https://jiyuanfeng.github.io/SyNDock" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->project<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2311.14580" class="hover:underline" target="_blank" rel="noopener noreferrer">Large Language Models as Automated Aligners for Benchmarking Vision-Language Models</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">ICLR24</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji*, Chongjian Ge*, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2311.14580" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/AutoBench" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a><a href="https://jiyuanfeng.github.io/AutoBench" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->project<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2303.17559" class="hover:underline" target="_blank" rel="noopener noreferrer">DDP: Diffusion Model for Dense Visual Prediction</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">ICCV23</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji*, Zhe Chen*, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2303.17559" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/DDP" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2201.09637" class="hover:underline" target="_blank" rel="noopener noreferrer">DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">AAAI23</div><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Oral</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji*, Lu Zhang*, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang, Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, Houtim Lai, Wei Liu, Junzhou Huang, Shuigeng Zhou, Ping Luo, Peilin Zhao, Yatao Bian</p><div class="text-pretty"><a href="https://arxiv.org/abs/2201.09637" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/DrugOOD" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a><a href="https://jiyuanfeng.github.io/DrugOOD" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->project<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2206.08023" class="hover:underline" target="_blank" rel="noopener noreferrer">AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">NeurIPS22</div><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Oral</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2206.08023" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/AMOS" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a><a href="https://jiyuanfeng.github.io/AMOS" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->project<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2106.14385" class="hover:underline" target="_blank" rel="noopener noreferrer">Multi-Compound Transformer for Accurate Biomedical Image Segmentation</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">MICCAI21</div><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">EA</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji, Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting Zhang, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2106.14385" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/MCT" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2009.07501" class="hover:underline" target="_blank" rel="noopener noreferrer">UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">MICCAI20</div><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">EA</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji, Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping Luo</p><div class="text-pretty"><a href="https://arxiv.org/abs/2009.07501" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/UXNet" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/2010.11725" class="hover:underline" target="_blank" rel="noopener noreferrer">RANet: Region Attention Network for Semantic Segmentation</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">Neuips2020</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Dingguo Shen*, Yuanfeng Ji*, Ping Li, Yi Wang, Di Lin</p><div class="text-pretty"><a href="https://arxiv.org/abs/2010.11725" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a><a href="https://github.com/JiYuanFeng/RANet" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->code<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/1909.05651" class="hover:underline" target="_blank" rel="noopener noreferrer">PRSNet: Part Relation and Selection Network for Bone Age Assessment</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">MICCAI2019</div><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">EA</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Yuanfeng Ji, Hao Chen, Dan Lin, Xiaohua Wu, Di Lin</p><div class="text-pretty"><a href="https://arxiv.org/abs/1909.05651" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a></div></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-4"><div class="flex flex-col space-y-1.5"><h3 class="font-semibold leading-normal inline-flex items-center gap-x-1"><a href="https://arxiv.org/abs/1807.00583" class="hover:underline" target="_blank" rel="noopener noreferrer">Multi-Scale Context Intertwining for Semantic Segmentation</a><span class="inline-flex gap-x-1"><div class="inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5">ECCV18</div></span></h3></div><div class="text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]"><p class="text-pretty mb-2">Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, Hui Huang</p><div class="text-pretty"><a href="https://arxiv.org/abs/1807.00583" class="text-blue-600 hover:underline text-xs mr-2" target="_blank" rel="noopener noreferrer">[<!-- -->paper<!-- -->]</a></div></div></div></div></section><section class="flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16"><h2 class="text-xl font-bold">Awards &amp; Achievements</h2><div class="-mx-3 grid grid-cols-1 gap-3 print:grid-cols-2 print:gap-2 md:grid-cols-2 lg:grid-cols-2"><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-3"> <div class="flex flex-col space-y-1.5 relative"><h3 class="text-base font-semibold leading-normal"> <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge" class="hover:underline" target="_blank" rel="noopener noreferrer">Kaggle RSNA Pneumonia Detection Challenge</a></h3></div><div class="text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]"> <p class="text-pretty">Ranked 5th (Gold Medal)</p></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-3"> <div class="flex flex-col space-y-1.5 relative"><h3 class="text-base font-semibold leading-normal"> <a href="https://cocodataset.org/#panoptic-2019" class="hover:underline" target="_blank" rel="noopener noreferrer">COCO 2019 Panoptic Segmentation Task</a></h3></div><div class="text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]"> <p class="text-pretty">Ranked 3rd</p></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-3"> <div class="flex flex-col space-y-1.5 relative"><h3 class="text-base font-semibold leading-normal"> <a href="https://challenge2018.isic-archive.com/" class="hover:underline" target="_blank" rel="noopener noreferrer">MICCAI 2018 ISIC Skin Lesion Segmentation Challenge</a></h3></div><div class="text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]"> <p class="text-pretty">Ranked 3rd</p></div></div><div class="bg-card text-card-foreground border border-gray-100 rounded-lg p-3"> <div class="flex flex-col space-y-1.5 relative"><h3 class="text-base font-semibold leading-normal"> <a href="https://www.kaggle.com/c/human-protein-atlas-image-classification" class="hover:underline" target="_blank" rel="noopener noreferrer">Kaggle Human Protein Atlas Image Classification Challenge</a></h3></div><div class="text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]"> <p class="text-pretty">Ranked 87th (Silver Medal)</p></div></div></div></section></section><p class="fixed bottom-0 left-0 right-0 hidden border-t border-t-muted bg-white p-1 text-center text-sm text-muted-foreground print:hidden xl:block">Press<!-- --> <kbd class="pointer-events-none inline-flex h-5 select-none items-center gap-1 rounded border bg-muted px-1.5 font-mono text-[10px] font-medium text-muted-foreground opacity-100"><span class="text-xs">⌘</span>J</kbd> <!-- -->to open the command menu</p><button class="items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-10 w-10 fixed bottom-4 right-4 flex rounded-full shadow-2xl print:hidden xl:hidden"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-command my-6 size-6"><path d="M15 6v12a3 3 0 1 0 3-3H6a3 3 0 1 0 3 3V6a3 3 0 1 0-3 3h12a3 3 0 1 0-3-3"></path></svg></button></main><script src="/JiYuanFeng.github.io/_next/static/chunks/webpack-a3b8e77fa4d566ad.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/JiYuanFeng.github.io/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/JiYuanFeng.github.io/_next/static/css/d64a1b622cbccaf5.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[5751,[],\"\"]\n5:I[9733,[\"193\",\"static/chunks/193-5cbb6b05bc1adb61.js\",\"931\",\"static/chunks/app/page-207b02275482d296.js\"],\"Button\"]\n6:I[420,[\"193\",\"static/chunks/193-5cbb6b05bc1adb61.js\",\"931\",\"static/chunks/app/page-207b02275482d296.js\"],\"Avatar\"]\n7:I[420,[\"193\",\"static/chunks/193-5cbb6b05bc1adb61.js\",\"931\",\"static/chunks/app/page-207b02275482d296.js\"],\"AvatarImage\"]\n8:I[420,[\"193\",\"static/chunks/193-5cbb6b05bc1adb61.js\",\"931\",\"static/chunks/app/page-207b02275482d296.js\"],\"AvatarFallback\"]\n9:I[6954,[\"193\",\"static/chunks/193-5cbb6b05bc1adb61.js\",\"931\",\"static/chunks/app/page-207b02275482d296.js\"],\"CommandMenu\"]\na:I[9275,[],\"\"]\nb:I[1343,[],\"\"]\nc:I[3897,[\"185\",\"static/chunks/app/layout-81ae2f8f2c0f9613.js\"],\"Analytics\"]\ne:I[6130,[],\"\"]\nf:[]\n"])</script><script>self.__next_f.push([1,"0:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/JiYuanFeng.github.io/_next/static/css/d64a1b622cbccaf5.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"$L3\",null,{\"buildId\":\"hGSk1_i7NlWJq0Cgn6BX7\",\"assetPrefix\":\"/JiYuanFeng.github.io\",\"initialCanonicalUrl\":\"/\",\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L4\",[\"$\",\"main\",null,{\"className\":\"container relative mx-auto scroll-my-12 overflow-auto p-4 print:p-12 md:p-16\",\"children\":[[\"$\",\"section\",null,{\"className\":\"mx-auto w-full max-w-2xl space-y-8 bg-white print:space-y-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center justify-between\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 space-y-1.5\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-2xl font-bold\",\"children\":\"Yuanfeng Ji\"}],[\"$\",\"p\",null,{\"className\":\"max-w-md text-pretty font-mono text-sm text-muted-foreground print:text-[12px]\",\"children\":\"Researcher specializing in artificial intelligence applications in medicine, focused on advancing reliable and impactful healthcare technologies.\"}],[\"$\",\"p\",null,{\"className\":\"max-w-md items-center text-pretty font-mono text-xs text-muted-foreground\",\"children\":[\"$\",\"a\",null,{\"className\":\"inline-flex gap-x-1.5 align-baseline leading-none hover:underline\",\"href\":\"https://www.google.com/maps/place/California\",\"target\":\"_blank\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-globe size-3\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"path\",\"13o1zl\",{\"d\":\"M12 2a14.5 14.5 0 0 0 0 20 14.5 14.5 0 0 0 0-20\"}],[\"$\",\"path\",\"9i4pu4\",{\"d\":\"M2 12h20\"}],\"$undefined\"]}],\"California, USA, PT\"]}]}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 pt-1 font-mono text-sm text-muted-foreground print:hidden\",\"children\":[[\"$\",\"$L5\",null,{\"className\":\"size-8\",\"variant\":\"outline\",\"size\":\"icon\",\"asChild\":true,\"children\":[\"$\",\"a\",null,{\"href\":\"mailto:yfj@stanford.edu\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-mail size-4\",\"children\":[[\"$\",\"rect\",\"18n3k1\",{\"width\":\"20\",\"height\":\"16\",\"x\":\"2\",\"y\":\"4\",\"rx\":\"2\"}],[\"$\",\"path\",\"1ocrg3\",{\"d\":\"m22 7-8.97 5.7a1.94 1.94 0 0 1-2.06 0L2 7\"}],\"$undefined\"]}]}]}],[\"$\",\"$L5\",null,{\"className\":\"size-8\",\"variant\":\"outline\",\"size\":\"icon\",\"asChild\":true,\"children\":[\"$\",\"a\",null,{\"href\":\"tel:6504441771\",\"children\":[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-phone size-4\",\"children\":[[\"$\",\"path\",\"foiqr5\",{\"d\":\"M22 16.92v3a2 2 0 0 1-2.18 2 19.79 19.79 0 0 1-8.63-3.07 19.5 19.5 0 0 1-6-6 19.79 19.79 0 0 1-3.07-8.67A2 2 0 0 1 4.11 2h3a2 2 0 0 1 2 1.72 12.84 12.84 0 0 0 .7 2.81 2 2 0 0 1-.45 2.11L8.09 9.91a16 16 0 0 0 6 6l1.27-1.27a2 2 0 0 1 2.11-.45 12.84 12.84 0 0 0 2.81.7A2 2 0 0 1 22 16.92z\"}],\"$undefined\"]}]}]}],[[\"$\",\"$L5\",\"GitHub\",{\"className\":\"size-8\",\"variant\":\"outline\",\"size\":\"icon\",\"asChild\":true,\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/JiYuanFeng\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"size-4\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12\"}]}]}]}],[\"$\",\"$L5\",\"LinkedIn\",{\"className\":\"size-8\",\"variant\":\"outline\",\"size\":\"icon\",\"asChild\":true,\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/yuanfeng-ji/\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"size-4\",\"children\":[[\"$\",\"title\",null,{\"children\":\"LinkedIn\"}],[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z\"}]]}]}]}],[\"$\",\"$L5\",\"X\",{\"className\":\"size-8\",\"variant\":\"outline\",\"size\":\"icon\",\"asChild\":true,\"children\":[\"$\",\"a\",null,{\"href\":\"https://x.com/YuanfengJi\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"className\":\"size-4\",\"children\":[[\"$\",\"title\",null,{\"children\":\"X\"}],[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z\"}]]}]}]}]]]}],[\"$\",\"div\",null,{\"className\":\"hidden flex-col gap-x-1 font-mono text-sm text-muted-foreground print:flex print:text-[12px]\",\"children\":[[\"$\",\"a\",null,{\"href\":\"mailto:yfj@stanford.edu\",\"children\":[\"$\",\"span\",null,{\"className\":\"underline\",\"children\":\"yfj@stanford.edu\"}]}],[\"$\",\"a\",null,{\"href\":\"tel:6504441771\",\"children\":[\"$\",\"span\",null,{\"className\":\"underline\",\"children\":\"6504441771\"}]}]]}]]}],[\"$\",\"$L6\",null,{\"className\":\"size-28\",\"children\":[[\"$\",\"$L7\",null,{\"alt\":\"Yuanfeng Ji\",\"src\":\"src/data/image.png\"}],[\"$\",\"$L8\",null,{\"children\":\"YJ\"}]]}]]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"About\"}],[\"$\",\"p\",null,{\"className\":\"text-pretty font-mono text-sm text-muted-foreground print:text-[12px]\",\"children\":\"As a postdoctoral researcher at Stanford University, I am dedicated to pioneering AI solutions in medical imaging and digital pathology. My work is driven by a commitment to developing robust and trustworthy systems that can contribute to advancements in healthcare diagnostics and treatment planning.\"}]]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"News\"}],[\"$\",\"div\",null,{\"className\":\"relative max-w-3xl mx-auto\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"border-l-2 border-gray-300 ml-4\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":\"2024-11-11\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.xiaohongshu.com/explore/672a20b7000000001b02e05d?app_platform=android\u0026ignoreEngage=true\u0026app_version=8.58.0\u0026share_from_user_hidden=true\u0026xsec_source=app_share\u0026type=normal\u0026xsec_token=CBbHMhqciUrFq55JAzY43nmv_0rOqQQMnJubUEmchn_mw=\u0026author_share=1\u0026xhsshare=WeixinSession\u0026shareRedId=N0pGNUhLRUs2NzUyOTgwNjY0OTc5NUxL\u0026apptime=1730912776\u0026share_id=b0d1e8b70625499aa9c7e1898cd5f932\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Hiring Internship Positions for Medical AGI Research\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Our team is looking for talented interns to join us in exploring Medical AGI. For detailed information, please see the link. If you're interested and passionate, feel free to reach out directly!\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":\"2024-11-1\"}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://profiles.stanford.edu/yuanfeng-ji\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Joined LiLab as a Postdoctoral Researcher\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"I completed my Ph.D. in August 2024 and have joined Li Lab at Stanford University as a postdoctoral researcher.\"}]]}]]}]]}]]}]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Work Experience\"}],[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[\"$\",\"div\",null,{\"className\":\"border-l-2 border-gray-300 ml-4\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2024\",\" - \",\"Present\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://profiles.stanford.edu/yuanfeng-ji\",\"children\":\"Stanford University\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Postdoctoral Researcher\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Engaged in AI applications for precision medicine under the guidance of Prof. Ruijiang Li.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"1\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2023\",\" - \",\"2024\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://profiles.stanford.edu/yuanfeng-ji\",\"children\":\"Stanford University\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Visiting Student Researcher\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Engaged in AI applications for precision medicine under the guidance of Prof. Ruijiang Li.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"2\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2022\",\" - \",\"2023\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://www.noahlab.com.hk/\",\"children\":\"Huawei Noah's Ark Lab\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Research Intern\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Developed AI models for predicting cancer treatment outcomes, focusing on precision medicine.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"3\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2021\",\" - \",\"2022\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://ai.tencent.com/ailab/en/index\",\"children\":\"Tencent AI Lab\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Research Intern\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Led the development of a DrugAI dataset and benchmark for out-of-distribution generalization; developed multi-protein docking algorithms incorporating graph-based deep learning techniques.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"4\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2019\",\" - \",\"2020\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://www.sensetime.com/\",\"children\":\"SenseTime Research\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Research Intern\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Developed automated machine learning algorithms for medical image analysis; led the creation of a multi-site abdominal organ segmentation dataset and benchmark.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"5\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2018\",\" - \",\"2019\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://www.imsightmed.com/\",\"children\":\"Imsight Medical Technology\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Deep Learning Researcher\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Led the development of CAD products implemented in several institutions in Hong Kong, including a chest X-ray diagnostic system detecting 17 lung diseases and a sequencing algorithm optimizing diagnostic queues at medical facilities.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}],[\"$\",\"div\",\"6\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2018\",\" - \",\"2019\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":[\"$\",\"a\",null,{\"className\":\"hover:underline\",\"href\":\"https://vcc.tech/\",\"children\":\"Visual Computing Research Center, Shenzhen University\"}]}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Research Assistant\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Under the supervision of Prof. Hui Huang and Prof. Di Lin, contributed to research on semantic segmentation.\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-x-1 mt-1\",\"children\":[]}]]}]]}]}]]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Education\"}],[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[\"$\",\"div\",null,{\"className\":\"border-l-2 border-gray-300 ml-4\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2014\",\" - \",\"2018\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":\"Shenzhen University\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Bachelor's Degree in Electronic Information Engineering\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2019\",\" - \",\"2020\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":\"City University of Hong Kong\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Master's Degree in Electronic Information Engineering\"}]]}],[\"$\",\"div\",\"2\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2020\",\" - \",\"2022\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":\"The University of Hong Kong\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"MPhil in Computer Science\"}]]}],[\"$\",\"div\",\"3\",{\"className\":\"relative mb-8 ml-8\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-gray-500\",\"children\":[\"2022\",\" - \",\"2024\"]}],[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold mt-1\",\"children\":\"The University of Hong Kong\"}],[\"$\",\"p\",null,{\"className\":\"text-xs mt-1 text-gray-700\",\"children\":\"Ph.D. Candidate in Computer Science\"}]]}]]}]}]]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Projects\"}],[\"$\",\"div\",null,{\"className\":\"-mx-3 grid grid-cols-1 gap-3 print:grid-cols-3 print:gap-2 md:grid-cols-2 lg:grid-cols-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-semibold tracking-tight text-base\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2303.17559\",\"target\":\"_blank\",\"className\":\"inline-flex items-center gap-1 hover:underline\",\"children\":[\"DDP: Diffusion Model for Dense Visual Prediction\",\" \",[\"$\",\"span\",null,{\"className\":\"size-1 rounded-full bg-green-500\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"hidden font-mono text-xs underline print:visible\",\"children\":\"arxiv.orgabs/2303.17559\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground font-mono text-xs print:text-[10px]\",\"children\":\"Developed a framework for dense visual predictions based on the conditional diffusion pipeline, following a 'noise-to-map' generative paradigm.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-sm text-muted-foreground mt-auto flex\",\"children\":[\"$\",\"div\",null,{\"className\":\"mt-2 flex flex-wrap gap-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Research Project\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Python\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"PyTorch\"}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-semibold tracking-tight text-base\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://neurips.cc/virtual/2022/poster/55771\",\"target\":\"_blank\",\"className\":\"inline-flex items-center gap-1 hover:underline\",\"children\":[\"AMOS: A Large-Scale Abdominal Multi-Organ Benchmark\",\" \",[\"$\",\"span\",null,{\"className\":\"size-1 rounded-full bg-green-500\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"hidden font-mono text-xs underline print:visible\",\"children\":\"neurips.ccvirtual/2022/poster/55771\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground font-mono text-xs print:text-[10px]\",\"children\":\"Created a comprehensive benchmark for abdominal multi-organ segmentation, facilitating advancements in medical image analysis.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-sm text-muted-foreground mt-auto flex\",\"children\":[\"$\",\"div\",null,{\"className\":\"mt-2 flex flex-wrap gap-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Research Project\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Python\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"TensorFlow\"}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"rounded-lg bg-card text-card-foreground flex flex-col overflow-hidden border border-muted p-3\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"div\",null,{\"className\":\"space-y-1\",\"children\":[[\"$\",\"h3\",null,{\"className\":\"font-semibold tracking-tight text-base\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2201.09637\",\"target\":\"_blank\",\"className\":\"inline-flex items-center gap-1 hover:underline\",\"children\":[\"DrugOOD: Out-of-Distribution Dataset Curator and Benchmark\",\" \",[\"$\",\"span\",null,{\"className\":\"size-1 rounded-full bg-green-500\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"hidden font-mono text-xs underline print:visible\",\"children\":\"arxiv.orgabs/2201.09637\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground font-mono text-xs print:text-[10px]\",\"children\":\"Developed a dataset curator and benchmark for AI-aided drug discovery, focusing on affinity prediction problems with noisy annotations.\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-sm text-muted-foreground mt-auto flex\",\"children\":[\"$\",\"div\",null,{\"className\":\"mt-2 flex flex-wrap gap-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Research Project\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"Python\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 px-1 py-0 text-[10px] print:px-1 print:py-0.5 print:text-[8px] print:leading-tight\",\"children\":\"PyTorch\"}]]}]}]]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Publications\"}],[\"$\",\"div\",null,{\"className\":\"-mx-3 grid grid-cols-1 gap-3 print:grid-cols-1 print:gap-2 md:grid-cols-1 lg:grid-cols-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2410.20723\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Tech Report\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Chongjian Ge, Chenfeng Xu, Yuanfeng Ji, Chensheng Peng, Masayoshi Tomizuka, Ping Luo, Mingyu Ding, Varun Jampani, Wei Zhan\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2410.20723\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"project\",{\"href\":\"https://chongjiange.github.io/compgs.html\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"project\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2410.11761\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Tech Report\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Ying Chen*, Guoan Wang*, Yuanfeng Ji*#, Yanjun Li, Jin Ye, Tianbin Li, Bin Zhang, Nana Pei, Rongshan Yu, Yu Qiao, Junjun He#\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2410.11761\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2305.15156\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"SyNDock: N Rigid Protein Docking via Learnable Group Synchronization\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Tech Report\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji, Yatao Bian, Guoji Fu, Peilin Zhao, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2305.15156\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/SyNDock\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}],[\"$\",\"a\",\"project\",{\"href\":\"https://jiyuanfeng.github.io/SyNDock\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"project\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2311.14580\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Large Language Models as Automated Aligners for Benchmarking Vision-Language Models\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"ICLR24\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji*, Chongjian Ge*, Weikai Kong, Enze Xie, Zhengying Liu, Zhengguo Li, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2311.14580\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/AutoBench\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}],[\"$\",\"a\",\"project\",{\"href\":\"https://jiyuanfeng.github.io/AutoBench\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"project\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2303.17559\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"DDP: Diffusion Model for Dense Visual Prediction\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"ICCV23\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji*, Zhe Chen*, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2303.17559\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/DDP\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2201.09637\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"AAAI23\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Oral\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji*, Lu Zhang*, Jiaxiang Wu, Bingzhe Wu, Lanqing Li, Long-Kai Huang, Tingyang Xu, Yu Rong, Jie Ren, Ding Xue, Houtim Lai, Wei Liu, Junzhou Huang, Shuigeng Zhou, Ping Luo, Peilin Zhao, Yatao Bian\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2201.09637\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/DrugOOD\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}],[\"$\",\"a\",\"project\",{\"href\":\"https://jiyuanfeng.github.io/DrugOOD\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"project\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2206.08023\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"NeurIPS22\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Oral\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2206.08023\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/AMOS\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}],[\"$\",\"a\",\"project\",{\"href\":\"https://jiyuanfeng.github.io/AMOS\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"project\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2106.14385\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Multi-Compound Transformer for Accurate Biomedical Image Segmentation\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"MICCAI21\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"EA\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji, Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting Zhang, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2106.14385\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/MCT\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2009.07501\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"UXNet: Searching Multi-level Feature Aggregation for 3D Medical Image Segmentation\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"MICCAI20\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"EA\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji, Ruimao Zhang, Zhen Li, Jiamin Ren, Shaoting Zhang, Ping Luo\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2009.07501\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/UXNet\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/2010.11725\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"RANet: Region Attention Network for Semantic Segmentation\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"Neuips2020\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Dingguo Shen*, Yuanfeng Ji*, Ping Li, Yi Wang, Di Lin\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/2010.11725\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}],[\"$\",\"a\",\"code\",{\"href\":\"https://github.com/JiYuanFeng/RANet\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"code\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1909.05651\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"PRSNet: Part Relation and Selection Network for Bone Age Assessment\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"MICCAI2019\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"EA\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Yuanfeng Ji, Hao Chen, Dan Lin, Xiaohua Wu, Di Lin\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/1909.05651\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5\",\"children\":[\"$\",\"h3\",null,{\"className\":\"font-semibold leading-normal inline-flex items-center gap-x-1\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://arxiv.org/abs/1807.00583\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Multi-Scale Context Intertwining for Semantic Segmentation\"}],[\"$\",\"span\",null,{\"className\":\"inline-flex gap-x-1\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2 py-0.5 font-semibold font-mono transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-nowrap border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/60 align-middle text-xs print:text-[8px] print:leading-tight print:px-1 print:py-0.5\",\"children\":\"ECCV18\"}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-2 text-xs print:text-[10px]\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-pretty mb-2\",\"children\":\"Di Lin, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, Hui Huang\"}],[\"$\",\"div\",null,{\"className\":\"text-pretty\",\"children\":[[\"$\",\"a\",\"paper\",{\"href\":\"https://arxiv.org/abs/1807.00583\",\"className\":\"text-blue-600 hover:underline text-xs mr-2\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":[\"[\",\"paper\",\"]\"]}]]}]]}]]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"flex min-h-0 flex-col gap-y-3 print-force-new-page scroll-mb-16\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-bold\",\"children\":\"Awards \u0026 Achievements\"}],[\"$\",\"div\",null,{\"className\":\"-mx-3 grid grid-cols-1 gap-3 print:grid-cols-2 print:gap-2 md:grid-cols-2 lg:grid-cols-2\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-3\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5 relative\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold leading-normal\",\"children\":[\" \",[\"$\",\"a\",null,{\"href\":\"https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Kaggle RSNA Pneumonia Detection Challenge\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]\",\"children\":[\" \",[\"$\",\"p\",null,{\"className\":\"text-pretty\",\"children\":\"Ranked 5th (Gold Medal)\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-3\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5 relative\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold leading-normal\",\"children\":[\" \",[\"$\",\"a\",null,{\"href\":\"https://cocodataset.org/#panoptic-2019\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"COCO 2019 Panoptic Segmentation Task\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]\",\"children\":[\" \",[\"$\",\"p\",null,{\"className\":\"text-pretty\",\"children\":\"Ranked 3rd\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-3\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5 relative\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold leading-normal\",\"children\":[\" \",[\"$\",\"a\",null,{\"href\":\"https://challenge2018.isic-archive.com/\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"MICCAI 2018 ISIC Skin Lesion Segmentation Challenge\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]\",\"children\":[\" \",[\"$\",\"p\",null,{\"className\":\"text-pretty\",\"children\":\"Ranked 3rd\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"bg-card text-card-foreground border border-gray-100 rounded-lg p-3\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-col space-y-1.5 relative\",\"children\":[\"$\",\"h3\",null,{\"className\":\"text-base font-semibold leading-normal\",\"children\":[\" \",[\"$\",\"a\",null,{\"href\":\"https://www.kaggle.com/c/human-protein-atlas-image-classification\",\"className\":\"hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Kaggle Human Protein Atlas Image Classification Challenge\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"text-pretty font-mono text-muted-foreground mt-1 text-xs print:text-[10px]\",\"children\":[\" \",[\"$\",\"p\",null,{\"className\":\"text-pretty\",\"children\":\"Ranked 87th (Silver Medal)\"}]]}]]}]]}]]}]]}],[\"$\",\"$L9\",null,{\"links\":[{\"url\":\"https://jiyuanfeng.github.io/\",\"title\":\"Personal Website\"},{\"url\":\"https://github.com/JiYuanFeng\",\"title\":\"GitHub\"},{\"url\":\"https://www.linkedin.com/in/yuanfeng-ji/\",\"title\":\"LinkedIn\"},{\"url\":\"https://x.com/YuanfengJi\",\"title\":\"X\"}]}]]}]],null],null]},[[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__className_25a186\",\"children\":[[\"$\",\"body\",null,{\"children\":[\"$\",\"$La\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":null}]}],[\"$\",\"$Lc\",null,{}]]}],null],null],\"couldBeIntercepted\":false,\"initialHead\":[false,\"$Ld\"],\"globalErrorComponent\":\"$e\",\"missingSlots\":\"$Wf\"}]]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Yuanfeng Ji | Researcher specializing in artificial intelligence applications in medicine, focused on advancing reliable and impactful healthcare technologies.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"As a postdoctoral researcher at Stanford University, I am dedicated to pioneering AI solutions in medical imaging and digital pathology. My work is driven by a commitment to developing robust and trustworthy systems that can contribute to advancements in healthcare diagnostics and treatment planning.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/JiYuanFeng.github.io/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"5\",{\"rel\":\"apple-touch-icon\",\"href\":\"/JiYuanFeng.github.io/apple-icon.png?d70e5c41a56c84c2\",\"type\":\"image/png\",\"sizes\":\"180x180\"}],[\"$\",\"meta\",\"6\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>